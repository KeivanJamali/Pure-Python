{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "models = {\"Google\": [\"gemma2-9b-it\", \"gemma-7b-it\"],\n",
    "          \"Groq\": [\"llama3-groq-70b-8192-tool-use-preview\", \"llama3-groq-8b-8192-tool-use-preview\"],\n",
    "          \"Meta\": [\"llama-3.1-70b-versatile\", \"llama-3.1-8b-instant\", \"llama-3.2-1b-preview\", \"llama-3.2-3b-preview\", \"llama-3.2-11b-vision-preview\", \"llama-3.2-90b-vision-preview\", \"llama-guard-3-8b\", \"llama3-70b-8192\", \"llama3-8b-8192\"],\n",
    "          \"Mistral\": [\"mixtral-8x7b-32768\"],\n",
    "          \"OpenAI\": [\"whisper-large-v3\", \"whisper-large-v3-turbo\"]}\n",
    "llm = ChatGroq(model=models[\"Meta\"][0], temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an AI language model assistant. Your task is to generate five \\ndifferent versions of the given user question to retrieve relavant documents from a vector \\ndatabase. By generating mulitple perspectives on the user quesion, your goal is to help \\nthe user overcome some of the limitations of the distance-based similarity search.\\nProvide these alternative questions seprated by newlines. Original question: {question} '), additional_kwargs={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relavant documents from a vector \n",
    "database. By generating mulitple perspectives on the user quesion, your goal is to help \n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions seprated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "multiquery_prompt = ChatPromptTemplate.from_template(template)\n",
    "multiquery_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the following question based on this context:\\n{context}\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_template = \"\"\"Answer the following question based on this context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# pdf_path = r\"D:\\All Python\\Pure-Python\\P4\\06-PromptEngineering\\Project 3 - OpenAI\\TP-ClassNote-6.pdf\"\n",
    "# loader = UnstructuredPDFLoader(file_path=pdf_path)\n",
    "# pdf_docs = loader.load()\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_path=(\"https://blog.langchain.dev/deconstructing-rag/\"),\n",
    "                       bs_kwargs=dict(\n",
    "                           parse_only=bs4.SoupStrainer(\n",
    "                               class_=(\"article-header section\", \"article-main section\")\n",
    "                           )\n",
    "                       ))\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,\n",
    "                                               chunk_overlap=50,\n",
    "                                               separators=\"\\n\")\n",
    "blog_docs_splitted = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "vectorsote = Chroma.from_documents(documents=blog_docs_splitted,\n",
    "                                   embedding=HuggingFaceEmbeddings())\n",
    "\n",
    "retriever = vectorsote.as_retriever()\n",
    "print(f\"number of docs: {vectorsote._collection.count()}\")\n",
    "retriever.search_kwargs[\"k\"] = vectorsote._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_multi_queries():\n",
    "    chain  = (\n",
    "        multiquery_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "    return chain    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of dumps and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"name\": \"Bob\", \"age\": 25}', '{\"name\": \"Alice\", \"age\": 30}', '{\"name\": \"Charlie\", \"age\": 35}']\n",
      "[{'name': 'Bob', 'age': 25}, {'name': 'Alice', 'age': 30}, {'name': 'Charlie', 'age': 35}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads  # Using dumps from langchain.load\n",
    "\n",
    "# Example input: a list of lists with dictionary documents\n",
    "documents = [\n",
    "    [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}],\n",
    "    [{\"name\": \"Alice\", \"age\": 30}],  # Duplicate document\n",
    "    [{\"name\": \"Charlie\", \"age\": 35}]\n",
    "]\n",
    "\n",
    "# Function to serialize and flatten the documents\n",
    "def get_unique_union1(documents: list[list]):\n",
    "    # Serialize each document to a string format and flatten the list\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Use set to keep only unique serialized documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return unique_docs\n",
    "\n",
    "def get_unique_union2(documents: list[list]):\n",
    "    flattened_docs = [loads(doc) for doc in documents]\n",
    "    return flattened_docs\n",
    "\n",
    "# Run the function\n",
    "unique_documents = get_unique_union1(documents)\n",
    "new = get_unique_union2(unique_documents)\n",
    "print(unique_documents)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    \n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def query(query, retriever):\n",
    "    retrieval_chain = (generate_multi_queries() \n",
    "                       | retriever.map()\n",
    "                       | get_unique_union)\n",
    "    # docs = retrieval_chain.invoke({\"question\": quesion})\n",
    "    # display(len(docs))\n",
    "    # return docs\n",
    "    final_rag_chain = ({\"context\": retrieval_chain, \"question\": RunnablePassthrough()} \n",
    "                       | rag_prompt\n",
    "                       | llm\n",
    "                       | StrOutputParser())\n",
    "    return final_rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Retrieval is a core component of the new LLM (Large Language Model) operating system. It involves retrieving information from a data source (or sources) and loading it into the context window of the LLM, which is then used in LLM output generation, a process typically called retrieval augmented generation (RAG)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "answer = query(\"What is retrivial?\", retriever)\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
