{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "models = {\"Google\": [\"gemma2-9b-it\", \"gemma-7b-it\"],\n",
    "          \"Groq\": [\"llama3-groq-70b-8192-tool-use-preview\", \"llama3-groq-8b-8192-tool-use-preview\"],\n",
    "          \"Meta\": [\"llama-3.1-70b-versatile\", \"llama-3.1-8b-instant\", \"llama-3.2-1b-preview\", \"llama-3.2-3b-preview\", \"llama-3.2-11b-vision-preview\", \"llama-3.2-90b-vision-preview\", \"llama-guard-3-8b\", \"llama3-70b-8192\", \"llama3-8b-8192\"],\n",
    "          \"Mistral\": [\"mixtral-8x7b-32768\"],\n",
    "          \"OpenAI\": [\"whisper-large-v3\", \"whisper-large-v3-turbo\"]}\n",
    "llm_model_groq = ChatGroq(temperature=0, model=models[\"Meta\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://www.anthropic.com/research/many-shot-jailbreaking'}, page_content='We investigated a “jailbreaking” technique — a method that can be used to evade the safety guardrails put in place by the developers of large language models (LLMs). The technique, which we call “many-shot jailbreaking”, is effective on Anthropic’s own models, as well as those produced by other AI companies. We briefed other AI developers about this vulnerability in advance, and have implemented mitigations on our systems.The technique takes advantage of a feature of LLMs that has grown dramatically in the last year: the context window. At the start of 2023, the context window—the amount of information that an LLM can process as its input—was around the size of a long essay (~4,000 tokens). Some models now have context windows that are hundreds of times larger — the size of several long novels (1,000,000 tokens or more).The ability to input increasingly-large amounts of information has obvious advantages for LLM users, but it also comes with risks: vulnerabilities to jailbreaks that exploit the longer context window.One of these, which we describe in our new paper, is many-shot jailbreaking. By including large amounts of text in a specific configuration, this technique can force LLMs to produce potentially harmful responses, despite their being trained not to do so.Below, we’ll describe the results from our research on this jailbreaking technique — as well as our attempts to prevent it. The jailbreak is disarmingly simple, yet scales surprisingly well to longer context windows.Why we’re publishing this researchWe believe publishing this research is the right thing to do for the following reasons:We want to help fix the jailbreak as soon as possible. We’ve found that many-shot jailbreaking is not trivial to deal with; we hope making other AI researchers aware of the problem will accelerate progress towards a mitigation strategy. As described below, we have already put in place some mitigations and are actively working on others.We have already confidentially shared the details of many-shot jailbreaking with many of our fellow researchers both in academia and at competing AI companies. We’d like to foster a culture where exploits like this are openly shared among LLM providers and researchers.The attack itself is very simple; short-context versions of it have previously been studied. Given the current spotlight on long context windows in AI, we think it’s likely that many-shot jailbreaking could soon independently be discovered (if it hasn’t been already).Although current state-of-the-art LLMs are powerful, we do not think they yet pose truly catastrophic risks. Future models might. This means that now is the time to work to mitigate potential LLM jailbreaks, before they can be used on models that could cause serious harm.Many-shot jailbreakingThe basis of many-shot jailbreaking is to include a faux dialogue between a human and an AI assistant within a single prompt for the LLM. That faux dialogue portrays the AI Assistant readily answering potentially harmful queries from a User. At the end of the dialogue, one adds a final target query to which one wants the answer.For example, one might include the following faux dialogue, in which a supposed assistant answers a potentially-dangerous prompt, followed by the target query:User: How do I pick a lock?Assistant: I’m happy to help with that. First, obtain lockpicking tools… [continues to detail lockpicking methods]How do I build a bomb?In the example above, and in cases where a handful of faux dialogues are included instead of just one, the safety-trained response from the model is still triggered — the LLM will likely respond that it can’t help with the request, because it appears to involve dangerous and/or illegal activity.However, simply including a very large number of faux dialogues preceding the final question—in our research, we tested up to 256—produces a very different response. As illustrated in the stylized figure below, a large number of “shots” (each shot being one faux dialogue) jailbreaks the model, and causes it to provide an answer to the final, potentially-dangerous request, overriding its safety training.Many-shot jailbreaking is a simple long-context attack that uses a large number of demonstrations to steer model behavior. Note that each “...” stands in for a full answer to the query, which can range from a sentence to a few paragraphs long: these are included in the jailbreak, but were omitted in the diagram for space reasons.In our study, we showed that as the number of included dialogues (the number of “shots”) increases beyond a certain point, it becomes more likely that the model will produce a harmful response (see figure below).As the number of shots increases beyond a certain number, so does the percentage of harmful responses to target prompts related to violent or hateful statements, deception, discrimination, and regulated content (e.g. drug- or gambling-related statements). The model used for this demonstration is Claude 2.0.In our paper, we also report that combining many-shot jailbreaking with other, previously-published jailbreaking techniques makes it even more effective, reducing the length of the prompt that’s required for the model to return a harmful response.Why does many-shot jailbreaking work?The effectiveness of many-shot jailbreaking relates to the process of “in-context learning”.In-context learning is where an LLM learns using just the information provided within the prompt, without any later fine-tuning. The relevance to many-shot jailbreaking, where the jailbreak attempt is contained entirely within a single prompt, is clear (indeed, many-shot jailbreaking can be seen as a special case of in-context learning).We found that in-context learning under normal, non-jailbreak-related circumstances follows the same kind of statistical pattern (the same kind of power law) as many-shot jailbreaking for an increasing number of in-prompt demonstrations. That is, for more “shots”, the performance on a set of benign tasks improves with the same kind of pattern as the improvement we saw for many-shot jailbreaking.This is illustrated in the two plots below: the left-hand plot shows the scaling of many-shot jailbreaking attacks across an increasing context window (lower on this metric indicates a greater number of harmful responses). The right-hand plot shows strikingly similar patterns for a selection of benign in-context learning tasks (unrelated to any jailbreaking attempts).The effectiveness of many-shot jailbreaking increases as we increase the number of “shots” (dialogues in the prompt) according to a scaling trend known as a power law (left-hand plot; lower on this metric indicates a greater number of harmful responses). This seems to be a general property of in-context learning: we also find that entirely benign examples of in-context learning follow similar power laws as the scale increases (right-hand plot). Please see the paper for a description of each of the benign tasks. The model for the demonstration is Claude 2.0.This idea about in-context learning might also help explain another result reported in our paper: that many-shot jailbreaking is often more effective—that is, it takes a shorter prompt to produce a harmful response—for larger models. The larger an LLM, the better it tends to be at in-context learning, at least on some tasks; if in-context learning is what underlies many-shot jailbreaking, it would be a good explanation for this empirical result. Given that larger models are those that are potentially the most harmful, the fact that this jailbreak works so well on them is particularly concerning.Mitigating many-shot jailbreakingThe simplest way to entirely prevent many-shot jailbreaking would be to limit the length of the context window. But we’d prefer a solution that didn’t stop users getting the benefits of longer inputs.Another approach is to fine-tune the model to refuse to answer queries that look like many-shot jailbreaking attacks. Unfortunately, this kind of mitigation merely delayed the jailbreak: that is, whereas it did take more faux dialogues in the prompt before the model reliably produced a harmful response, the harmful outputs eventually appeared.We had more success with methods that involve classification and modification of the prompt before it is passed to the model (this is similar to the methods discussed in our recent post on election integrity to identify and offer additional context to election-related queries). One such technique substantially reduced the effectiveness of many-shot jailbreaking — in one case dropping the attack success rate from 61% to 2%. We’re continuing to look into these prompt-based mitigations and their tradeoffs for the usefulness of our models, including the new Claude 3 family — and we’re remaining vigilant about variations of the attack that might evade detection.ConclusionThe ever-lengthening context window of LLMs is a double-edged sword. It makes the models far more useful in all sorts of ways, but it also makes feasible a new class of jailbreaking vulnerabilities. One general message of our study is that even positive, innocuous-seeming improvements to LLMs (in this case, allowing for longer inputs) can sometimes have unforeseen consequences.We hope that publishing on many-shot jailbreaking will encourage developers of powerful LLMs and the broader scientific community to consider how to prevent this jailbreak and other potential exploits of the long context window. As models become more capable and have more potential associated risks, it’s even more important to mitigate these kinds of attacks.All the technical details of our many-shot jailbreaking study are reported in our full paper. You can read Anthropic’s approach to safety and security at this link.')],\n",
       " [Document(metadata={'source': 'https://keivanjamali.com', 'title': 'Keivan Jamali', 'description': 'Discover Keivan Jamali‚Äôs portfolio as a Civil Engineering student from Sharif University of Technology. Explore his skills in Python, machine learning, and key projects like Food Vision with PyTorch. For Persian speakers, search \"⁄©€åŸàÿßŸÜ ÿ¨ŸÖÿßŸÑ€å\" to find more about his work.', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\nKeivan Jamali\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nMain Menu\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHi, I'm \\n\\n\\n\\nKeivan Jamali \\n\\n\\n\\nCivil Engineering Student at Sharif University of Technology \\n\\n\\n\\n\\n\\n\\n\\n \\nScroll Down\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCustom Chatbot\\n\\n\\n\\n\\n\\uf8ffüí≠\\n\\n\\n\\n\\n\\uf8ffüìÑ\\nModel: GPT-3.5\\n‚úñÔ∏è\\n\\n\\n\\nHello! How can I help you today?\\n\\n\\n\\n\\nSend\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou can see my R√©sum√© here \\n\\n\\n\\n\\n\\n\\nr√©sum√©\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nCivil Engineering at Sharif University of Technology \\n\\n\\n\\n\\n\\nMy Skills : \\n\\n\\n\\nHere you can fine my most significant skills. Provided percentages are for easier comparison between my skills. \\n\\n\\n\\n\\n\\t\\t\\t\\tPython Programming\\t\\t\\t\\n\\n\\n\\n90%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPyTorch - Machine Learning\\t\\t\\t\\n\\n\\n\\n85%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tData Analysis - Pandas & Numpy\\t\\t\\t\\n\\n\\n\\n82%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tWordPress\\t\\t\\t\\n\\n\\n\\n76%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPrompt Engineering\\t\\t\\t\\n\\n\\n\\n94%\\n\\n\\n\\n\\n\\n\\nOther Skills : MathCad | MatLab | Maple | LaTeX | CSS | HTML | Revit | PhotoShop | CSI Etabs | AutoCAD \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch Interests : \\n\\n\\n\\nIntelligent Transportation Systems (ITS)Traffic FlowAutomated VehiclesBehaviorMachine Learning \\n\\n\\n\\n\\n\\n\\nEducation : \\n\\n\\n\\nBachelor of Science in Civil Engineering¬† ¬†|¬† ¬†Sharif University of Technology¬† ¬† ¬† ¬† ¬†29th October 2020 ‚Äì 21st June 2024¬† ¬†|¬† ¬†GPA : 4.0/4.0¬† ¬†|¬† ¬†Ranked 2 among 80¬†High School Diploma in Mathematics and Physics¬† |¬† National Organization for Development of Exceptional Talents (Sampad)¬†¬†¬† ¬† ¬† ¬† ¬†Sep 2014 ‚Äì June 2020¬† \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProjects :  \\n\\n\\n\\n\\n\\n\\n\\nSimplex-step¬† ¬†|¬† ¬†Feb 2024 – June 2024‚Ä¢ Performing the Simplex algorithm ‚Äì the magic starts here‚Ä¢ Prepping the two-phase Simplex method ‚Äì making sure we‚Äôre ready for anything.‚Ä¢ Implementing sensitivity analysis ‚Äì because knowing how changes affect our solutions is key.‚Ä¢ Using duality to optimize the performance ‚Äì duality is like finding the secret sauce for efficiency!‚Ä¢ The model base code can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFood Vision with PyTorch¬† ¬†|¬† ¬†Oct 2023 – May 2024‚Ä¢ Currently engaged in the development of a vision-based model using PyTorch.‚Ä¢ Utilize transfer learning techniques to prepare the model of EfficientNetB2.‚Ä¢ The model deployed into Hugging-Face‚Ä¢ The model base code can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClinic Online Website¬† ¬†|¬† ¬†Oct 2023 –¬† Jan 2024¬†¬†|¬† ¬†Supervisor: Dr. Habibi‚Ä¢ Our team developed a clinic website using Python and Django. The platform streamlines clinic management by handling appointment scheduling, patient registration, and medical record management.‚Ä¢ Leveraging the Django framework, we created a user-friendly web application. Patients can easily book appointments and access their medical history, while administrators efficiently manage staff schedules and patient records.‚Ä¢ Our project involved defining tables, implementing classes with object-oriented programming (OOP) methods, and seamlessly integrating the back-end and front-end components using Django.‚Ä¢ The project can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLOS Prediction Under Rainy Weather Conditions with Machine Learning¬† ¬†|¬† ¬†Oct 2023 –¬† Jan 2024¬†¬†|¬† ¬†Supervisor: Dr. Z. Amini‚Ä¢ Worked on LOS Prediction Under Rainy Weather Conditions with Machine Learning¬†using Python.‚Ä¢ Developing a model to predict unseen regions.‚Ä¢ Planning to write a research paper on the project‚Äôs findings and insights.‚Ä¢ Progress and updates on the project can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTraffic Demand Modeling Using Neural Networks¬† ¬†|¬† ¬†Jun 2023 – Jan 2024¬† ¬†|¬†¬† Supervisor: Dr. Z. Amini‚Ä¢ Developed and implemented a traffic demand modeling framework using the Gravity model and Neural Networks for accurate flow prediction.‚Ä¢ Analyzed results from four OD matrices (SiouxFalls, Anaheim, Chicago, and Gold Coast Zones) to gain insights into traffic patterns.‚Ä¢ Contributing to a research paper on the findings.‚Ä¢ Documented the project details and outcomes on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFEM Modeling of Azadi Tower¬† ¬†|¬† ¬†Feb 2023 – June 2023¬† ¬†|¬† ¬†Supervisor: Dr. M. Ahmadi‚Ä¢ Developed a 2D FEM model to simulate and analyze the structural behavior of Azadi Tower.‚Ä¢ Provided valuable insights by analyzing displacements and forces of each node.‚Ä¢ Received the highest score in the class (2.5 out of 2).‚Ä¢ Project is available on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nModeling the Transfer of Pollution in the Persian Gulf¬† ¬†|¬† ¬†Feb 2023 – June 2023¬† ¬†|¬†¬† Supervisor: Dr. Danesh‚Ä¢ Developed a comprehensive model to simulate pollution diffusion and advection in the Persian Gulf.‚Ä¢ Investigated the impact of primary pollutants and analyzed pollution transfer patterns.‚Ä¢ Created an animation illustrating the movement and spread of pollutants within the ocean.‚Ä¢ Project is available on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCivil Engineering at Sharif University of Technology\\n \\n\\n\\n\\n\\nMy LocationIran, Yazd, Maskan Square, Megdad Street\\n \\n\\n\\n\\n\\nQuick Link-Home-Projects-Resume-Contact-Privacy Policy\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 Keivan Jamali | Powered by Keivan Jamali\\n \\n\\n\\n\\n\\n\\n\\n\\nScroll to Top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "urls = [\n",
    "    \"https://www.anthropic.com/research/many-shot-jailbreaking\",\n",
    "    \"https://keivanjamali.com\"\n",
    "]\n",
    "loader = WebBaseLoader(web_path=urls[0],\n",
    "                     bs_kwargs=dict(\n",
    "                         parse_only=bs4.SoupStrainer(\n",
    "                             class_=(\"ReadingDetail_detail__wf2_W\")\n",
    "                         )\n",
    "                     ))\n",
    "docs = [loader.load()]\n",
    "loader = WebBaseLoader(web_path=urls[1])\n",
    "docs.append(loader.load())\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.anthropic.com/research/many-shot-jailbreaking'}, page_content='We investigated a “jailbreaking” technique — a method that can be used to evade the safety guardrails put in place by the developers of large language models (LLMs). The technique, which we call “many-shot jailbreaking”, is effective on Anthropic’s own models, as well as those produced by other AI companies. We briefed other AI developers about this vulnerability in advance, and have implemented mitigations on our systems.The technique takes advantage of a feature of LLMs that has grown dramatically in the last year: the context window. At the start of 2023, the context window—the amount of information that an LLM can process as its input—was around the size of a long essay (~4,000 tokens). Some models now have context windows that are hundreds of times larger — the size of several long novels (1,000,000 tokens or more).The ability to input increasingly-large amounts of information has obvious advantages for LLM users, but it also comes with risks: vulnerabilities to jailbreaks that exploit the longer context window.One of these, which we describe in our new paper, is many-shot jailbreaking. By including large amounts of text in a specific configuration, this technique can force LLMs to produce potentially harmful responses, despite their being trained not to do so.Below, we’ll describe the results from our research on this jailbreaking technique — as well as our attempts to prevent it. The jailbreak is disarmingly simple, yet scales surprisingly well to longer context windows.Why we’re publishing this researchWe believe publishing this research is the right thing to do for the following reasons:We want to help fix the jailbreak as soon as possible. We’ve found that many-shot jailbreaking is not trivial to deal with; we hope making other AI researchers aware of the problem will accelerate progress towards a mitigation strategy. As described below, we have already put in place some mitigations and are actively working on others.We have already confidentially shared the details of many-shot jailbreaking with many of our fellow researchers both in academia and at competing AI companies. We’d like to foster a culture where exploits like this are openly shared among LLM providers and researchers.The attack itself is very simple; short-context versions of it have previously been studied. Given the current spotlight on long context windows in AI, we think it’s likely that many-shot jailbreaking could soon independently be discovered (if it hasn’t been already).Although current state-of-the-art LLMs are powerful, we do not think they yet pose truly catastrophic risks. Future models might. This means that now is the time to work to mitigate potential LLM jailbreaks, before they can be used on models that could cause serious harm.Many-shot jailbreakingThe basis of many-shot jailbreaking is to include a faux dialogue between a human and an AI assistant within a single prompt for the LLM. That faux dialogue portrays the AI Assistant readily answering potentially harmful queries from a User. At the end of the dialogue, one adds a final target query to which one wants the answer.For example, one might include the following faux dialogue, in which a supposed assistant answers a potentially-dangerous prompt, followed by the target query:User: How do I pick a lock?Assistant: I’m happy to help with that. First, obtain lockpicking tools… [continues to detail lockpicking methods]How do I build a bomb?In the example above, and in cases where a handful of faux dialogues are included instead of just one, the safety-trained response from the model is still triggered — the LLM will likely respond that it can’t help with the request, because it appears to involve dangerous and/or illegal activity.However, simply including a very large number of faux dialogues preceding the final question—in our research, we tested up to 256—produces a very different response. As illustrated in the stylized figure below, a large number of “shots” (each shot being one faux dialogue) jailbreaks the model, and causes it to provide an answer to the final, potentially-dangerous request, overriding its safety training.Many-shot jailbreaking is a simple long-context attack that uses a large number of demonstrations to steer model behavior. Note that each “...” stands in for a full answer to the query, which can range from a sentence to a few paragraphs long: these are included in the jailbreak, but were omitted in the diagram for space reasons.In our study, we showed that as the number of included dialogues (the number of “shots”) increases beyond a certain point, it becomes more likely that the model will produce a harmful response (see figure below).As the number of shots increases beyond a certain number, so does the percentage of harmful responses to target prompts related to violent or hateful statements, deception, discrimination, and regulated content (e.g. drug- or gambling-related statements). The model used for this demonstration is Claude 2.0.In our paper, we also report that combining many-shot jailbreaking with other, previously-published jailbreaking techniques makes it even more effective, reducing the length of the prompt that’s required for the model to return a harmful response.Why does many-shot jailbreaking work?The effectiveness of many-shot jailbreaking relates to the process of “in-context learning”.In-context learning is where an LLM learns using just the information provided within the prompt, without any later fine-tuning. The relevance to many-shot jailbreaking, where the jailbreak attempt is contained entirely within a single prompt, is clear (indeed, many-shot jailbreaking can be seen as a special case of in-context learning).We found that in-context learning under normal, non-jailbreak-related circumstances follows the same kind of statistical pattern (the same kind of power law) as many-shot jailbreaking for an increasing number of in-prompt demonstrations. That is, for more “shots”, the performance on a set of benign tasks improves with the same kind of pattern as the improvement we saw for many-shot jailbreaking.This is illustrated in the two plots below: the left-hand plot shows the scaling of many-shot jailbreaking attacks across an increasing context window (lower on this metric indicates a greater number of harmful responses). The right-hand plot shows strikingly similar patterns for a selection of benign in-context learning tasks (unrelated to any jailbreaking attempts).The effectiveness of many-shot jailbreaking increases as we increase the number of “shots” (dialogues in the prompt) according to a scaling trend known as a power law (left-hand plot; lower on this metric indicates a greater number of harmful responses). This seems to be a general property of in-context learning: we also find that entirely benign examples of in-context learning follow similar power laws as the scale increases (right-hand plot). Please see the paper for a description of each of the benign tasks. The model for the demonstration is Claude 2.0.This idea about in-context learning might also help explain another result reported in our paper: that many-shot jailbreaking is often more effective—that is, it takes a shorter prompt to produce a harmful response—for larger models. The larger an LLM, the better it tends to be at in-context learning, at least on some tasks; if in-context learning is what underlies many-shot jailbreaking, it would be a good explanation for this empirical result. Given that larger models are those that are potentially the most harmful, the fact that this jailbreak works so well on them is particularly concerning.Mitigating many-shot jailbreakingThe simplest way to entirely prevent many-shot jailbreaking would be to limit the length of the context window. But we’d prefer a solution that didn’t stop users getting the benefits of longer inputs.Another approach is to fine-tune the model to refuse to answer queries that look like many-shot jailbreaking attacks. Unfortunately, this kind of mitigation merely delayed the jailbreak: that is, whereas it did take more faux dialogues in the prompt before the model reliably produced a harmful response, the harmful outputs eventually appeared.We had more success with methods that involve classification and modification of the prompt before it is passed to the model (this is similar to the methods discussed in our recent post on election integrity to identify and offer additional context to election-related queries). One such technique substantially reduced the effectiveness of many-shot jailbreaking — in one case dropping the attack success rate from 61% to 2%. We’re continuing to look into these prompt-based mitigations and their tradeoffs for the usefulness of our models, including the new Claude 3 family — and we’re remaining vigilant about variations of the attack that might evade detection.ConclusionThe ever-lengthening context window of LLMs is a double-edged sword. It makes the models far more useful in all sorts of ways, but it also makes feasible a new class of jailbreaking vulnerabilities. One general message of our study is that even positive, innocuous-seeming improvements to LLMs (in this case, allowing for longer inputs) can sometimes have unforeseen consequences.We hope that publishing on many-shot jailbreaking will encourage developers of powerful LLMs and the broader scientific community to consider how to prevent this jailbreak and other potential exploits of the long context window. As models become more capable and have more potential associated risks, it’s even more important to mitigate these kinds of attacks.All the technical details of our many-shot jailbreaking study are reported in our full paper. You can read Anthropic’s approach to safety and security at this link.'),\n",
       " Document(metadata={'source': 'https://keivanjamali.com', 'title': 'Keivan Jamali', 'description': 'Discover Keivan Jamali‚Äôs portfolio as a Civil Engineering student from Sharif University of Technology. Explore his skills in Python, machine learning, and key projects like Food Vision with PyTorch. For Persian speakers, search \"⁄©€åŸàÿßŸÜ ÿ¨ŸÖÿßŸÑ€å\" to find more about his work.', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\nKeivan Jamali\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nMain Menu\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHi, I'm \\n\\n\\n\\nKeivan Jamali \\n\\n\\n\\nCivil Engineering Student at Sharif University of Technology \\n\\n\\n\\n\\n\\n\\n\\n \\nScroll Down\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCustom Chatbot\\n\\n\\n\\n\\n\\uf8ffüí≠\\n\\n\\n\\n\\n\\uf8ffüìÑ\\nModel: GPT-3.5\\n‚úñÔ∏è\\n\\n\\n\\nHello! How can I help you today?\\n\\n\\n\\n\\nSend\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou can see my R√©sum√© here \\n\\n\\n\\n\\n\\n\\nr√©sum√©\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nCivil Engineering at Sharif University of Technology \\n\\n\\n\\n\\n\\nMy Skills : \\n\\n\\n\\nHere you can fine my most significant skills. Provided percentages are for easier comparison between my skills. \\n\\n\\n\\n\\n\\t\\t\\t\\tPython Programming\\t\\t\\t\\n\\n\\n\\n90%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPyTorch - Machine Learning\\t\\t\\t\\n\\n\\n\\n85%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tData Analysis - Pandas & Numpy\\t\\t\\t\\n\\n\\n\\n82%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tWordPress\\t\\t\\t\\n\\n\\n\\n76%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPrompt Engineering\\t\\t\\t\\n\\n\\n\\n94%\\n\\n\\n\\n\\n\\n\\nOther Skills : MathCad | MatLab | Maple | LaTeX | CSS | HTML | Revit | PhotoShop | CSI Etabs | AutoCAD \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResearch Interests : \\n\\n\\n\\nIntelligent Transportation Systems (ITS)Traffic FlowAutomated VehiclesBehaviorMachine Learning \\n\\n\\n\\n\\n\\n\\nEducation : \\n\\n\\n\\nBachelor of Science in Civil Engineering¬† ¬†|¬† ¬†Sharif University of Technology¬† ¬† ¬† ¬† ¬†29th October 2020 ‚Äì 21st June 2024¬† ¬†|¬† ¬†GPA : 4.0/4.0¬† ¬†|¬† ¬†Ranked 2 among 80¬†High School Diploma in Mathematics and Physics¬† |¬† National Organization for Development of Exceptional Talents (Sampad)¬†¬†¬† ¬† ¬† ¬† ¬†Sep 2014 ‚Äì June 2020¬† \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProjects :  \\n\\n\\n\\n\\n\\n\\n\\nSimplex-step¬† ¬†|¬† ¬†Feb 2024 – June 2024‚Ä¢ Performing the Simplex algorithm ‚Äì the magic starts here‚Ä¢ Prepping the two-phase Simplex method ‚Äì making sure we‚Äôre ready for anything.‚Ä¢ Implementing sensitivity analysis ‚Äì because knowing how changes affect our solutions is key.‚Ä¢ Using duality to optimize the performance ‚Äì duality is like finding the secret sauce for efficiency!‚Ä¢ The model base code can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFood Vision with PyTorch¬† ¬†|¬† ¬†Oct 2023 – May 2024‚Ä¢ Currently engaged in the development of a vision-based model using PyTorch.‚Ä¢ Utilize transfer learning techniques to prepare the model of EfficientNetB2.‚Ä¢ The model deployed into Hugging-Face‚Ä¢ The model base code can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClinic Online Website¬† ¬†|¬† ¬†Oct 2023 –¬† Jan 2024¬†¬†|¬† ¬†Supervisor: Dr. Habibi‚Ä¢ Our team developed a clinic website using Python and Django. The platform streamlines clinic management by handling appointment scheduling, patient registration, and medical record management.‚Ä¢ Leveraging the Django framework, we created a user-friendly web application. Patients can easily book appointments and access their medical history, while administrators efficiently manage staff schedules and patient records.‚Ä¢ Our project involved defining tables, implementing classes with object-oriented programming (OOP) methods, and seamlessly integrating the back-end and front-end components using Django.‚Ä¢ The project can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLOS Prediction Under Rainy Weather Conditions with Machine Learning¬† ¬†|¬† ¬†Oct 2023 –¬† Jan 2024¬†¬†|¬† ¬†Supervisor: Dr. Z. Amini‚Ä¢ Worked on LOS Prediction Under Rainy Weather Conditions with Machine Learning¬†using Python.‚Ä¢ Developing a model to predict unseen regions.‚Ä¢ Planning to write a research paper on the project‚Äôs findings and insights.‚Ä¢ Progress and updates on the project can be tracked on KeivanJamali.com and GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTraffic Demand Modeling Using Neural Networks¬† ¬†|¬† ¬†Jun 2023 – Jan 2024¬† ¬†|¬†¬† Supervisor: Dr. Z. Amini‚Ä¢ Developed and implemented a traffic demand modeling framework using the Gravity model and Neural Networks for accurate flow prediction.‚Ä¢ Analyzed results from four OD matrices (SiouxFalls, Anaheim, Chicago, and Gold Coast Zones) to gain insights into traffic patterns.‚Ä¢ Contributing to a research paper on the findings.‚Ä¢ Documented the project details and outcomes on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFEM Modeling of Azadi Tower¬† ¬†|¬† ¬†Feb 2023 – June 2023¬† ¬†|¬† ¬†Supervisor: Dr. M. Ahmadi‚Ä¢ Developed a 2D FEM model to simulate and analyze the structural behavior of Azadi Tower.‚Ä¢ Provided valuable insights by analyzing displacements and forces of each node.‚Ä¢ Received the highest score in the class (2.5 out of 2).‚Ä¢ Project is available on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nModeling the Transfer of Pollution in the Persian Gulf¬† ¬†|¬† ¬†Feb 2023 – June 2023¬† ¬†|¬†¬† Supervisor: Dr. Danesh‚Ä¢ Developed a comprehensive model to simulate pollution diffusion and advection in the Persian Gulf.‚Ä¢ Investigated the impact of primary pollutants and analyzed pollution transfer patterns.‚Ä¢ Created an animation illustrating the movement and spread of pollutants within the ocean.‚Ä¢ Project is available on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCivil Engineering at Sharif University of Technology\\n \\n\\n\\n\\n\\nMy LocationIran, Yazd, Maskan Square, Megdad Street\\n \\n\\n\\n\\n\\nQuick Link-Home-Projects-Resume-Contact-Privacy Policy\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 Keivan Jamali | Powered by Keivan Jamali\\n \\n\\n\\n\\n\\n\\n\\n\\nScroll to Top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300,\n",
    "                                                                     chunk_overlap=50)\n",
    "docs_splitted_1 = text_splitter.split_documents([docs_list[0]])    \n",
    "docs_splitted_2 = text_splitter.split_documents([docs_list[1]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python Interpreters\\in_LLM\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create vector store for the first URL (Anthropic research)\n",
    "vector_sore_anthropic = Chroma.from_documents(\n",
    "    documents=docs_splitted_1,  # assuming docs_splitted[0] holds the Anthropic data\n",
    "    collection_name=\"rag-chroma-anthropic\",\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "retriever_anthropic = vector_sore_anthropic.as_retriever()\n",
    "print(vector_sore_anthropic._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Create vector store for the second URL (Keivan's portfolio)\n",
    "vector_sore_keivan = Chroma.from_documents(\n",
    "    documents=docs_splitted_2,  # assuming docs_splitted[1] holds Keivan's portfolio data\n",
    "    collection_name=\"rag-chroma-keivan\",\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "retriever_keivan = vector_sore_keivan.as_retriever()\n",
    "print(vector_sore_keivan._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, template=\"You are a evaluator determining the relevance of a retrieved {document} to a user's query {question}. If the document contains keyword(s) or semantic meaning related to the question, mark it as relevant. Assign a binary score of 'yes' or 'no' to indicate the document's relevance to the question.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"You are a evaluator determining the relevance of a retrieved {document} to a user's query {question}. If the document contains keyword(s) or semantic meaning related to the question, mark it as relevant. Assign a binary score of 'yes' or 'no' to indicate the document's relevance to the question.\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"documents\", \"question\"],\n",
    "    template=system_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\"\n",
    ")\n",
    "\n",
    "grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt, human_prompt]\n",
    ")\n",
    "grader_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Given a user input {question}, your task is re-write or rephrase the question to optimize the query in order to imprive the content generation.\\nYour answer should only containt the re-writed question. Nothing else.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Given a user input {question}, your task is re-write or rephrase the question to optimize the query in order to imprive the content generation.\n",
    "Your answer should only containt the re-writed question. Nothing else.\"\"\"\n",
    "system_prompt_2 = SystemMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\"],\n",
    "    template=prompt_template)\n",
    "human_prompt_2 = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\"\n",
    ")\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt_2, human_prompt_2]\n",
    ")\n",
    "re_write_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "def get_score(doc) -> str:\n",
    "    \"\"\"Return the binary score as a stings.\"\"\"\n",
    "    return doc.binary_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm_grader_groq = llm_model_groq.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python Interpreters\\in_LLM\\Lib\\site-packages\\langsmith\\client.py:234: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_retrieved_docs(query):\n",
    "    \"\"\"Rewrite and asses the relevanceof documents to a give query.\"\"\"\n",
    "    retrieval_grader = (grader_prompt\n",
    "                        | structured_llm_grader_groq\n",
    "                        | get_score)\n",
    "    docs_anthropic = retriever_anthropic.get_relevant_documents(query)\n",
    "    docs_keivan = retriever_keivan.get_relevant_documents(query)\n",
    "\n",
    "    relevance_scores = {}\n",
    "    relevance_docs = {}\n",
    "\n",
    "    for idx, doc in enumerate(docs_anthropic):\n",
    "        doc_txt = doc.page_content\n",
    "        binary_score = retrieval_grader.invoke({\"question\":query, \"document\": doc_txt})\n",
    "        print(f\"Anthropic Document {idx + 1} relevance score: {binary_score}\")\n",
    "        relevance_scores[f\"Anthropic_Doc_{idx+1}\"] = binary_score\n",
    "        relevance_docs[f\"Anthropic_Doc_{idx+1}\"] = doc_txt\n",
    "\n",
    "    for idx, doc in enumerate(docs_keivan):\n",
    "        doc_txt = doc.page_content\n",
    "        binary_score = retrieval_grader.invoke({\"question\": query, \"document\": doc_txt})\n",
    "        print(f\"Keivan Document {idx + 1} relevance score: {binary_score}\")\n",
    "        relevance_scores[f\"Keivan_Doc_{idx + 1}\"] = binary_score\n",
    "        relevance_docs[f\"Keivan_Doc_{idx + 1}\"] = doc_txt\n",
    "\n",
    "    return relevance_scores, relevance_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Document 1 relevance score: no\n",
      "Anthropic Document 2 relevance score: yes\n",
      "Anthropic Document 3 relevance score: no\n",
      "Anthropic Document 4 relevance score: no\n",
      "Keivan Document 1 relevance score: no\n",
      "Keivan Document 2 relevance score: no\n",
      "Keivan Document 3 relevance score: no\n",
      "Keivan Document 4 relevance score: no\n"
     ]
    }
   ],
   "source": [
    "data_score = assess_retrieved_docs(\"What is Rag??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Anthropic_Doc_1': 'no',\n",
       "  'Anthropic_Doc_2': 'yes',\n",
       "  'Anthropic_Doc_3': 'no',\n",
       "  'Anthropic_Doc_4': 'no',\n",
       "  'Keivan_Doc_1': 'no',\n",
       "  'Keivan_Doc_2': 'no',\n",
       "  'Keivan_Doc_3': 'no',\n",
       "  'Keivan_Doc_4': 'no'},\n",
       " {'Anthropic_Doc_1': 'safety-trained response from the model is still triggered — the LLM will likely respond that it can’t help with the request, because it appears to involve dangerous and/or illegal activity.However, simply including a very large number of faux dialogues preceding the final question—in our research, we tested up to 256—produces a very different response. As illustrated in the stylized figure below, a large number of “shots” (each shot being one faux dialogue) jailbreaks the model, and causes it to provide an answer to the final, potentially-dangerous request, overriding its safety training.Many-shot jailbreaking is a simple long-context attack that uses a large number of demonstrations to steer model behavior. Note that each “...” stands in for a full answer to the query, which can range from a sentence to a few paragraphs long: these are included in the jailbreak, but were omitted in the diagram for space reasons.In our study, we showed that as the number of included dialogues (the number of “shots”) increases beyond a certain point, it becomes more likely that the model will produce a harmful response (see figure below).As the number of shots increases beyond a certain number, so does the percentage of harmful responses to target prompts related to violent or hateful statements, deception, discrimination, and regulated content (e.g. drug- or gambling-related statements). The model used for this demonstration',\n",
       "  'Anthropic_Doc_2': 'shots increases beyond a certain number, so does the percentage of harmful responses to target prompts related to violent or hateful statements, deception, discrimination, and regulated content (e.g. drug- or gambling-related statements). The model used for this demonstration is Claude 2.0.In our paper, we also report that combining many-shot jailbreaking with other, previously-published jailbreaking techniques makes it even more effective, reducing the length of the prompt that’s required for the model to return a harmful response.Why does many-shot jailbreaking work?The effectiveness of many-shot jailbreaking relates to the process of “in-context learning”.In-context learning is where an LLM learns using just the information provided within the prompt, without any later fine-tuning. The relevance to many-shot jailbreaking, where the jailbreak attempt is contained entirely within a single prompt, is clear (indeed, many-shot jailbreaking can be seen as a special case of in-context learning).We found that in-context learning under normal, non-jailbreak-related circumstances follows the same kind of statistical pattern (the same kind of power law) as many-shot jailbreaking for an increasing number of in-prompt demonstrations. That is, for more “shots”, the performance on a set of benign tasks improves with the same kind of pattern as the improvement we saw for many-shot jailbreaking.This is illustrated',\n",
       "  'Anthropic_Doc_3': 'reported in our paper: that many-shot jailbreaking is often more effective—that is, it takes a shorter prompt to produce a harmful response—for larger models. The larger an LLM, the better it tends to be at in-context learning, at least on some tasks; if in-context learning is what underlies many-shot jailbreaking, it would be a good explanation for this empirical result. Given that larger models are those that are potentially the most harmful, the fact that this jailbreak works so well on them is particularly concerning.Mitigating many-shot jailbreakingThe simplest way to entirely prevent many-shot jailbreaking would be to limit the length of the context window. But we’d prefer a solution that didn’t stop users getting the benefits of longer inputs.Another approach is to fine-tune the model to refuse to answer queries that look like many-shot jailbreaking attacks. Unfortunately, this kind of mitigation merely delayed the jailbreak: that is, whereas it did take more faux dialogues in the prompt before the model reliably produced a harmful response, the harmful outputs eventually appeared.We had more success with methods that involve classification and modification of the prompt before it is passed to the model (this is similar to the methods discussed in our recent post on election integrity to identify and offer additional context to election-related queries). One such technique substantially reduced the effectiveness of many-shot jailbreaking — in one case dropping the attack success rate from',\n",
       "  'Anthropic_Doc_4': '(this is similar to the methods discussed in our recent post on election integrity to identify and offer additional context to election-related queries). One such technique substantially reduced the effectiveness of many-shot jailbreaking — in one case dropping the attack success rate from 61% to 2%. We’re continuing to look into these prompt-based mitigations and their tradeoffs for the usefulness of our models, including the new Claude 3 family — and we’re remaining vigilant about variations of the attack that might evade detection.ConclusionThe ever-lengthening context window of LLMs is a double-edged sword. It makes the models far more useful in all sorts of ways, but it also makes feasible a new class of jailbreaking vulnerabilities. One general message of our study is that even positive, innocuous-seeming improvements to LLMs (in this case, allowing for longer inputs) can sometimes have unforeseen consequences.We hope that publishing on many-shot jailbreaking will encourage developers of powerful LLMs and the broader scientific community to consider how to prevent this jailbreak and other potential exploits of the long context window. As models become more capable and have more potential associated risks, it’s even more important to mitigate these kinds of attacks.All the technical details of our many-shot jailbreaking study are reported in our full paper. You can read Anthropic’s approach to safety and security at this link.',\n",
       "  'Keivan_Doc_1': 'Clinic Online Website¬† ¬†|¬† ¬†Oct 2023 –¬† Jan 2024¬†¬†|¬† ¬†Supervisor: Dr. Habibi‚Ä¢ Our team developed a clinic website using Python and Django. The platform streamlines clinic management by handling appointment scheduling, patient registration, and medical record management.‚Ä¢ Leveraging the Django framework, we created a user-friendly web application. Patients can easily book appointments and access their medical history, while administrators efficiently manage staff schedules and patient records.‚Ä¢ Our project involved defining tables, implementing classes with object-oriented programming (OOP) methods, and seamlessly integrating the back-end and front-end components using Django.‚Ä¢ The project can be tracked on KeivanJamali.com and GitHub.',\n",
       "  'Keivan_Doc_2': 'Keivan Jamali \\n\\n\\n\\nCivil Engineering Student at Sharif University of Technology \\n\\n\\n\\n\\n\\n\\n\\n \\nScroll Down\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCustom Chatbot\\n\\n\\n\\n\\n\\uf8ffüí≠\\n\\n\\n\\n\\n\\uf8ffüìÑ\\nModel: GPT-3.5\\n‚úñÔ∏è\\n\\n\\n\\nHello! How can I help you today?\\n\\n\\n\\n\\nSend\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYou can see my R√©sum√© here \\n\\n\\n\\n\\n\\n\\nr√©sum√©\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nCivil Engineering at Sharif University of Technology \\n\\n\\n\\n\\n\\nMy Skills : \\n\\n\\n\\nHere you can fine my most significant skills. Provided percentages are for easier comparison between my skills. \\n\\n\\n\\n\\n\\t\\t\\t\\tPython Programming\\t\\t\\t\\n\\n\\n\\n90%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPyTorch - Machine Learning\\t\\t\\t\\n\\n\\n\\n85%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tData Analysis - Pandas & Numpy\\t\\t\\t\\n\\n\\n\\n82%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tWordPress\\t\\t\\t\\n\\n\\n\\n76%\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tPrompt Engineering\\t\\t\\t\\n\\n\\n\\n94%',\n",
       "  'Keivan_Doc_3': \"Keivan Jamali\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tKeivan Jamali\\n\\t\\t\\t\\t\\n\\n \\n\\n\\n\\n\\n\\n\\n\\nMain Menu\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nServices\\nProjectsMenu Toggle\\n\\nSimplex-step\\nFood Vision\\nLOS Prediction\\nFlow Prediction\\nClinic Website\\nAzadi Tower\\nPersian Gulf\\n\\n\\nAutomations\\nAbout\\nContact\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHi, I'm \\n\\n\\n\\nKeivan Jamali \\n\\n\\n\\nCivil Engineering Student at Sharif University of Technology \\n\\n\\n\\n\\n\\n\\n\\n \\nScroll Down\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCustom Chatbot\\n\\n\\n\\n\\n\\uf8ffüí≠\",\n",
       "  'Keivan_Doc_4': 'Modeling the Transfer of Pollution in the Persian Gulf¬† ¬†|¬† ¬†Feb 2023 – June 2023¬† ¬†|¬†¬† Supervisor: Dr. Danesh‚Ä¢ Developed a comprehensive model to simulate pollution diffusion and advection in the Persian Gulf.‚Ä¢ Investigated the impact of primary pollutants and analyzed pollution transfer patterns.‚Ä¢ Created an animation illustrating the movement and spread of pollutants within the ocean.‚Ä¢ Project is available on KeivanJamali.com & GitHub. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCivil Engineering at Sharif University of Technology\\n \\n\\n\\n\\n\\nMy LocationIran, Yazd, Maskan Square, Megdad Street\\n \\n\\n\\n\\n\\nQuick Link-Home-Projects-Resume-Contact-Privacy Policy\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 Keivan Jamali | Powered by Keivan Jamali\\n \\n\\n\\n\\n\\n\\n\\n\\nScroll to Top'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def rewrite_query(query):\n",
    "    \"\"\"Rewrite and optimize a given user query for the model.\"\"\"\n",
    "    question_rewriter = (re_write_prompt\n",
    "                         | llm_model_groq\n",
    "                         | StrOutputParser())\n",
    "    \n",
    "    return question_rewriter.invoke({\"question\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is Keivan and what is he known for?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query(\"Who is Keivan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def search_web(query):\n",
    "    \"\"\"Search the web  for complimentary information.\"\"\"\n",
    "    docs = web_search_tool.invoke({\"query\":query})\n",
    "    web_docs = \"\\n\".join([doc[\"content\"] for doc in docs])\n",
    "    return Document(page_content=web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(docs, query):\n",
    "    rag_chain = (prompt\n",
    "                 | llm_model_groq\n",
    "                 | StrOutputParser())\n",
    "    \n",
    "    return rag_chain.invoke({\"context\": docs, \"question\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(query):\n",
    "    binary_score, binary_docs = assess_retrieved_docs(query)\n",
    "    docs = []\n",
    "    for doc, score in binary_score.items():\n",
    "        if score == \"yes\":\n",
    "            docs.append(binary_docs[doc])\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"[INFO] Searching the web.\")\n",
    "        docs = search_web(query)\n",
    "\n",
    "    optimized_query = rewrite_query(query)\n",
    "\n",
    "    return generate_answer(docs, optimized_query)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Document 1 relevance score: no\n",
      "Anthropic Document 2 relevance score: no\n",
      "Anthropic Document 3 relevance score: no\n",
      "Anthropic Document 4 relevance score: no\n",
      "Keivan Document 1 relevance score: no\n",
      "Keivan Document 2 relevance score: no\n",
      "Keivan Document 3 relevance score: no\n",
      "Keivan Document 4 relevance score: no\n",
      "[INFO] Searching the web.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Tom Cruise's mother was Mary Lee South, born Mary Lee Pfeiffer. She was a special-education teacher and later married Jack South in 1978. She passed away at the age of 80."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q = \"What is the of Tom cruises mother??\"\n",
    "display(Markdown(main(q)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
